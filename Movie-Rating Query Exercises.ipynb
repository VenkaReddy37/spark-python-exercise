{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101,Gone with the Wind,1939,Victor Fleming\n",
      "102,Star Wars,1977,George Lucas\n",
      "201,101,2,2011-01-22\n",
      "201,101,4,2011-01-27\n",
      "201,Sarah Martinez\n",
      "202,Daniel Lewis\n"
     ]
    }
   ],
   "source": [
    "# Creating RDD's\n",
    "\n",
    "movieRDD = sc.textFile(\"/Users/pravinkumar/Documents/Spark/testData/Movie-Rating\\ Query\\ Exercises/Movie.txt\")\n",
    "ratingRDD = sc.textFile(\"/Users/pravinkumar/Documents/Spark/testData/Movie-Rating\\ Query\\ Exercises/Rating.txt\")\n",
    "reviewerRDD = sc.textFile(\"/Users/pravinkumar/Documents/Spark/testData/Movie-Rating\\ Query\\ Exercises/Reviewer.txt\")\n",
    "\n",
    "for i in movieRDD.take(2): print(i)\n",
    "for i in ratingRDD.take(2): print(i)\n",
    "for i in reviewerRDD.take(2): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+------------------+----+\n",
      "|      director|mID|             title|year|\n",
      "+--------------+---+------------------+----+\n",
      "|Victor Fleming|101|Gone with the Wind|1939|\n",
      "|  George Lucas|102|         Star Wars|1977|\n",
      "+--------------+---+------------------+----+\n",
      "\n",
      "+---+---+----------+-----+\n",
      "|mID|rID|ratingDate|stars|\n",
      "+---+---+----------+-----+\n",
      "|101|201|2011-01-22|    2|\n",
      "|101|201|2011-01-27|    4|\n",
      "+---+---+----------+-----+\n",
      "\n",
      "+--------------+---+\n",
      "|          name|rID|\n",
      "+--------------+---+\n",
      "|Sarah Martinez|201|\n",
      "|  Daniel Lewis|202|\n",
      "+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating DF's\n",
    "from pyspark.sql import Row\n",
    "movieDF = movieRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: Row(mID = rec[0], title = rec[1], year = rec[2], \\\n",
    "                                                                       director = rec[3])).toDF()\n",
    "reviewerDF = reviewerRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: Row(rID = rec[0], name = rec[1])).toDF()\n",
    "ratingDF = ratingRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: Row(rID = rec[0], mID = rec[1], stars = rec[2], ratingDate = rec[3])).toDF()\n",
    "\n",
    "movieDF.registerTempTable(\"movie\")\n",
    "reviewerDF.registerTempTable(\"reviewer\")\n",
    "ratingDF.registerTempTable(\"rating\")\n",
    "\n",
    "movieDF.limit(2).show()\n",
    "ratingDF.limit(2).show()\n",
    "reviewerDF.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.T.\n",
      "Raiders of the Lost Ark\n",
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|                E.T.|\n",
      "|Raiders of the Lo...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 01 - Find the titles of all movies directed by Steven Spielberg.\n",
    "\n",
    "Query01RDD = movieRDD.map(lambda rec: rec.split(\",\")).filter(lambda rec: rec[3] == \"Steven Spielberg\").\\\n",
    "map(lambda rec: rec[1])\n",
    "for i in Query01RDD.collect(): print(i)\n",
    "    \n",
    "sqlContext.sql(\"select m.title from movie m where m.director = 'Steven Spielberg'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1939\n",
      "1937\n",
      "1981\n",
      "2009\n",
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|1981|\n",
      "|2009|\n",
      "|1939|\n",
      "|1937|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 02 - Find all years that have a movie that received a rating of 4 or 5, and sort them in increasing order.\n",
    "\n",
    "\n",
    "# Trimming down the rating RDD according to the Query\n",
    "ratingTrim = ratingRDD.map(lambda rec: rec.split(\",\")).filter(lambda rec: rec[2] == '4' or rec[2] == '5').\\\n",
    "map(lambda rec: (rec[1], rec[2]))\n",
    "movieTrim = movieRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[2]))\n",
    "\n",
    "Query02RDD = ratingTrim.join(movieTrim).map(lambda rec: rec[1][1]).distinct(numPartitions = 1)\n",
    "for i in Query02RDD.collect(): print(i)\n",
    "\n",
    "sqlContext.sql(\"select distinct m.year from movie m, rating r where m.mID = r.mID and r.stars in ('4', '5')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Wars\n",
      "Titanic\n",
      "+---------+\n",
      "|    title|\n",
      "+---------+\n",
      "|Star Wars|\n",
      "|  Titanic|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 03 - Find the titles of all movies that have no ratings. \n",
    "\n",
    "movieTrim = movieRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[1]))\n",
    "ratingTrim = ratingRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[1], rec[2]))\n",
    "Query03RDD = movieTrim.subtractByKey(ratingTrim).map(lambda rec: rec[1])\n",
    "\n",
    "for i in Query03RDD.collect(): print(i)\n",
    "    \n",
    "sqlContext.sql(\"select distinct m.title from movie m where m.mID not in (select r.mID from rating r)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daniel Lewis\n",
      "Chris Jackson\n",
      "+-------------+\n",
      "|         name|\n",
      "+-------------+\n",
      "|Chris Jackson|\n",
      "| Daniel Lewis|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 04 - Some reviewers didn't provide a date with their rating. \n",
    "# Find the names of all reviewers who have ratings with a NULL value for the date. \n",
    "\n",
    "ratingTrim = ratingRDD.map(lambda rec: rec.split(\",\")).filter(lambda rec: rec[3] == 'null').\\\n",
    "map(lambda rec: (rec[0], rec[3]))\n",
    "#for i in ratingTrim.collect(): print(i)\n",
    "reviewerTrim = reviewerRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[1]))\n",
    "#for i in reviewerTrim.collect(): print(i)\n",
    "\n",
    "Query04RDD = ratingTrim.join(reviewerTrim).map(lambda rec: rec[1][1])\n",
    "for i in Query04RDD.collect(): print(i)\n",
    "    \n",
    "sqlContext.sql(\"select e.name from reviewer e where e.rID in (select r.rID from rating r where r.ratingDate == 'null')\").\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ashley White', 'E.T.', '3', '2011-01-02']\n",
      "['Brittany Harris', 'Raiders of the Lost Ark', '2', '2011-01-30']\n",
      "['Brittany Harris', 'Raiders of the Lost Ark', '4', '2011-01-12']\n",
      "['Brittany Harris', 'The Sound of Music', '2', '2011-01-20']\n",
      "['Chris Jackson', 'E.T.', '2', '2011-01-22']\n",
      "['Chris Jackson', 'Raiders of the Lost Ark', '4', 'null']\n",
      "['Chris Jackson', 'The Sound of Music', '3', '2011-01-27']\n",
      "['Daniel Lewis', 'Snow White', '4', 'null']\n",
      "['Elizabeth Thomas', 'Avatar', '3', '2011-01-15']\n",
      "['Elizabeth Thomas', 'Snow White', '5', '2011-01-19']\n",
      "['James Cameron', 'Avatar', '5', '2011-01-20']\n",
      "['Mike Anderson', 'Gone with the Wind', '3', '2011-01-09']\n",
      "['Sarah Martinez', 'Gone with the Wind', '2', '2011-01-22']\n",
      "['Sarah Martinez', 'Gone with the Wind', '4', '2011-01-27']\n",
      "+----------------+--------------------+-----+----------+\n",
      "|            name|               title|stars|ratingDate|\n",
      "+----------------+--------------------+-----+----------+\n",
      "|    Ashley White|                E.T.|    3|2011-01-02|\n",
      "| Brittany Harris|Raiders of the Lo...|    2|2011-01-30|\n",
      "| Brittany Harris|Raiders of the Lo...|    4|2011-01-12|\n",
      "| Brittany Harris|  The Sound of Music|    2|2011-01-20|\n",
      "|   Chris Jackson|                E.T.|    2|2011-01-22|\n",
      "|   Chris Jackson|Raiders of the Lo...|    4|      null|\n",
      "|   Chris Jackson|  The Sound of Music|    3|2011-01-27|\n",
      "|    Daniel Lewis|          Snow White|    4|      null|\n",
      "|Elizabeth Thomas|              Avatar|    3|2011-01-15|\n",
      "|Elizabeth Thomas|          Snow White|    5|2011-01-19|\n",
      "|   James Cameron|              Avatar|    5|2011-01-20|\n",
      "|   Mike Anderson|  Gone with the Wind|    3|2011-01-09|\n",
      "|  Sarah Martinez|  Gone with the Wind|    2|2011-01-22|\n",
      "|  Sarah Martinez|  Gone with the Wind|    4|2011-01-27|\n",
      "+----------------+--------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 05 - Write a query to return the ratings data in a more readable format: \n",
    "# reviewer name, movie title, stars, and ratingDate. Also, sort the data, \n",
    "# first by reviewer name, then by movie title, and lastly by number of stars. \n",
    "\n",
    "movieTrim  = movieRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[1]))\n",
    "reviewerTrim = reviewerRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[1]))\n",
    "ratingTrim = ratingRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0],(rec[1], rec[2], rec[3])))\n",
    "ratingJoinReviewer = reviewerTrim.join(ratingTrim).map(lambda rec: (rec[1][1][0], (rec[1][0], rec[1][1][1], rec[1][1][2])))\n",
    "movieJoinrating = movieTrim.join(ratingJoinReviewer).map(lambda rec: ((rec[1][1][0], rec[1][0], rec[1][1][1]), rec[1][1][2]))\n",
    "\n",
    "Query05RDD = movieJoinrating.sortBy(lambda rec: rec[0], ascending = True).\\\n",
    "sortByKey(ascending = True).map(lambda rec: [rec[0][0], rec[0][1], rec[0][2], rec[1]])\n",
    "for i in Query05RDD.collect(): print(i)\n",
    "\n",
    "    \n",
    "sqlContext.sql(\"select e.name, m.title, r.stars, r.ratingDate from movie m, reviewer e, rating r where \\\n",
    "e.rID = r.rID and r.mID = m.mID order by e.name, m.title, r.stars asc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise @ https://lagunita.stanford.edu/courses/DB/SQL/SelfPaced/courseware/ch-sql/seq-exercise-sql_movie_query_core/\n",
    "\n",
    "# Here's the schema: \n",
    "\n",
    "# Movie ( mID, title, year, director ) \n",
    "# English: There is a movie with ID number mID, a title, a release year, and a director. \n",
    "\n",
    "# Reviewer ( rID, name ) \n",
    "# English: The reviewer with ID number rID has a certain name. \n",
    "\n",
    "# Rating ( rID, mID, stars, ratingDate ) \n",
    "# English: The reviewer rID gave the movie mID a number of stars rating (1-5) on a certain ratingDate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 210.0 failed 1 times, most recent failure: Lost task 0.0 in stage 210.0 (TID 2786, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-24-43270ac6cca4>\", line 28, in <lambda>\n  File \"<ipython-input-24-43270ac6cca4>\", line 15, in checkMovie\nTypeError: map() must have at least two arguments.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-24-43270ac6cca4>\", line 28, in <lambda>\n  File \"<ipython-input-24-43270ac6cca4>\", line 15, in checkMovie\nTypeError: map() must have at least two arguments.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-43270ac6cca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mQuery06RDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratingTrim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheckMovie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQuery06RDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \"\"\"\n\u001b[1;32m    807\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 210.0 failed 1 times, most recent failure: Lost task 0.0 in stage 210.0 (TID 2786, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-24-43270ac6cca4>\", line 28, in <lambda>\n  File \"<ipython-input-24-43270ac6cca4>\", line 15, in checkMovie\nTypeError: map() must have at least two arguments.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor105.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-24-43270ac6cca4>\", line 28, in <lambda>\n  File \"<ipython-input-24-43270ac6cca4>\", line 15, in checkMovie\nTypeError: map() must have at least two arguments.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Query 06 - For all cases where the same reviewer rated the same movie twice and gave it a higher \n",
    "# rating the second time, return the reviewer's name and the title of the movie.\n",
    "\n",
    "# Broadcasting datas\n",
    "movieTrim = movieRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[1]))\n",
    "movieBC = sc.broadcast(movieTrim.collectAsMap())\n",
    "ratingTrim = ratingRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], (rec[1], rec[2]))).groupByKey()\n",
    "ratingBC = sc.broadcast(ratingTrim.collectAsMap())\n",
    "reviewerTrim = reviewerRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[1]))\n",
    "reviewerBC = sc.broadcast(reviewerTrim.collectAsMap())\n",
    "\n",
    "# Quering Data\n",
    "def checkMovie(rec):\n",
    "    movieNStar = ratingBC.value.get(rec, None)\n",
    "    movieMap = map()\n",
    "    movieMap['0'] = '0'\n",
    "    for i in movieNStar:\n",
    "        if movieNStar == None:\n",
    "            break\n",
    "        movie, star = movieNStar\n",
    "        if int(movieMap[movie]) < int(star):\n",
    "            return [reviewerBC.value.get(rec, None), movieBC.value.get(movie, None)]\n",
    "        movieMap[movie] = star\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "Query06RDD = ratingTrim.map(lambda rec: checkMovie(rec[0]))\n",
    "for i in Query06RDD.collect(): print(i)\n",
    "\n",
    "\n",
    "#sqlContext.sql(\"select e.name, m.title from rating r, reviewer e, movie m where \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query 07 - For each movie that has at least one rating, find the highest number of stars that movie received. \n",
    "# Return the movie title and number of stars. Sort by movie title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query 08 - For each movie, return the title and the 'rating spread', that is, \n",
    "# the difference between highest and lowest ratings given to that movie. \n",
    "# Sort by rating spread from highest to lowest, then by movie title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query 09 - Find the difference between the average rating of movies released before 1980 \n",
    "# and the average rating of movies released after 1980. (Make sure to calculate the average rating \n",
    "# for each movie, then the average of those averages for movies before 1980 and movies after. \n",
    "# Don't just calculate the overall average rating before and after 1980.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query 10 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query 11 - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

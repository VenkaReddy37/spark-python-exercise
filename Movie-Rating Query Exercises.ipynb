{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101,Gone with the Wind,1939,Victor Fleming\n",
      "102,Star Wars,1977,George Lucas\n",
      "201,101,2,2011-01-22\n",
      "201,101,4,2011-01-27\n",
      "201,Sarah Martinez\n",
      "202,Daniel Lewis\n"
     ]
    }
   ],
   "source": [
    "# Creating RDD's\n",
    "\n",
    "movieRDD = sc.textFile(\"/Users/pravinkumar/Documents/Spark/testData/Movie-Rating\\ Query\\ Exercises/Movie.txt\")\n",
    "ratingRDD = sc.textFile(\"/Users/pravinkumar/Documents/Spark/testData/Movie-Rating\\ Query\\ Exercises/Rating.txt\")\n",
    "reviewerRDD = sc.textFile(\"/Users/pravinkumar/Documents/Spark/testData/Movie-Rating\\ Query\\ Exercises/Reviewer.txt\")\n",
    "\n",
    "for i in movieRDD.take(2): print(i)\n",
    "for i in ratingRDD.take(2): print(i)\n",
    "for i in reviewerRDD.take(2): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+------------------+----+\n",
      "|      director|mID|             title|year|\n",
      "+--------------+---+------------------+----+\n",
      "|Victor Fleming|101|Gone with the Wind|1939|\n",
      "|  George Lucas|102|         Star Wars|1977|\n",
      "+--------------+---+------------------+----+\n",
      "\n",
      "+---+---+----------+-----+\n",
      "|mID|rID|ratingDate|stars|\n",
      "+---+---+----------+-----+\n",
      "|101|201|2011-01-22|    2|\n",
      "|101|201|2011-01-27|    4|\n",
      "+---+---+----------+-----+\n",
      "\n",
      "+--------------+---+\n",
      "|          name|rID|\n",
      "+--------------+---+\n",
      "|Sarah Martinez|201|\n",
      "|  Daniel Lewis|202|\n",
      "+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating DF's\n",
    "from pyspark.sql import Row\n",
    "movieDF = movieRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: Row(mID = rec[0], title = rec[1], year = rec[2], \\\n",
    "                                                                       director = rec[3])).toDF()\n",
    "reviewerDF = reviewerRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: Row(rID = rec[0], name = rec[1])).toDF()\n",
    "ratingDF = ratingRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: Row(rID = rec[0], mID = rec[1], stars = rec[2], ratingDate = rec[3])).toDF()\n",
    "\n",
    "movieDF.registerTempTable(\"movie\")\n",
    "reviewerDF.registerTempTable(\"reviewer\")\n",
    "ratingDF.registerTempTable(\"rating\")\n",
    "\n",
    "movieDF.limit(2).show()\n",
    "ratingDF.limit(2).show()\n",
    "reviewerDF.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.T.\n",
      "Raiders of the Lost Ark\n",
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|                E.T.|\n",
      "|Raiders of the Lo...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 01 - Find the titles of all movies directed by Steven Spielberg.\n",
    "\n",
    "Query01RDD = movieRDD.map(lambda rec: rec.split(\",\")).filter(lambda rec: rec[3] == \"Steven Spielberg\").\\\n",
    "map(lambda rec: rec[1])\n",
    "for i in Query01RDD.collect(): print(i)\n",
    "    \n",
    "sqlContext.sql(\"select m.title from movie m where m.director = 'Steven Spielberg'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1939\n",
      "1937\n",
      "1981\n",
      "2009\n",
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|1981|\n",
      "|2009|\n",
      "|1939|\n",
      "|1937|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 02 - Find all years that have a movie that received a rating of 4 or 5, and sort them in increasing order.\n",
    "\n",
    "\n",
    "# Trimming down the rating RDD according to the Query\n",
    "ratingTrim = ratingRDD.map(lambda rec: rec.split(\",\")).filter(lambda rec: rec[2] == '4' or rec[2] == '5').\\\n",
    "map(lambda rec: (rec[1], rec[2]))\n",
    "movieTrim = movieRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[2]))\n",
    "\n",
    "Query02RDD = ratingTrim.join(movieTrim).map(lambda rec: rec[1][1]).distinct(numPartitions = 1)\n",
    "for i in Query02RDD.collect(): print(i)\n",
    "\n",
    "sqlContext.sql(\"select distinct m.year from movie m, rating r where m.mID = r.mID and r.stars in ('4', '5')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Wars\n",
      "Titanic\n",
      "+---------+\n",
      "|    title|\n",
      "+---------+\n",
      "|Star Wars|\n",
      "|  Titanic|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 03 - Find the titles of all movies that have no ratings. \n",
    "\n",
    "movieTrim = movieRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[1]))\n",
    "ratingTrim = ratingRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[1], rec[2]))\n",
    "Query03RDD = movieTrim.subtractByKey(ratingTrim).map(lambda rec: rec[1])\n",
    "\n",
    "for i in Query03RDD.collect(): print(i)\n",
    "    \n",
    "sqlContext.sql(\"select distinct m.title from movie m where m.mID not in (select r.mID from rating r)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daniel Lewis\n",
      "Chris Jackson\n",
      "+-------------+\n",
      "|         name|\n",
      "+-------------+\n",
      "|Chris Jackson|\n",
      "| Daniel Lewis|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 04 - Some reviewers didn't provide a date with their rating. \n",
    "# Find the names of all reviewers who have ratings with a NULL value for the date. \n",
    "\n",
    "ratingTrim = ratingRDD.map(lambda rec: rec.split(\",\")).filter(lambda rec: rec[3] == 'null').\\\n",
    "map(lambda rec: (rec[0], rec[3]))\n",
    "#for i in ratingTrim.collect(): print(i)\n",
    "reviewerTrim = reviewerRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[1]))\n",
    "#for i in reviewerTrim.collect(): print(i)\n",
    "\n",
    "Query04RDD = ratingTrim.join(reviewerTrim).map(lambda rec: rec[1][1])\n",
    "for i in Query04RDD.collect(): print(i)\n",
    "    \n",
    "sqlContext.sql(\"select e.name from reviewer e where e.rID in (select r.rID from rating r where r.ratingDate == 'null')\").\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Ashley White', 'E.T.', '3'), '2011-01-02')\n",
      "(('Brittany Harris', 'Raiders of the Lost Ark', '2'), '2011-01-30')\n",
      "(('Brittany Harris', 'Raiders of the Lost Ark', '4'), '2011-01-12')\n",
      "(('Brittany Harris', 'The Sound of Music', '2'), '2011-01-20')\n",
      "(('Chris Jackson', 'E.T.', '2'), '2011-01-22')\n",
      "(('Chris Jackson', 'Raiders of the Lost Ark', '4'), 'null')\n",
      "(('Chris Jackson', 'The Sound of Music', '3'), '2011-01-27')\n",
      "(('Daniel Lewis', 'Snow White', '4'), 'null')\n",
      "(('Elizabeth Thomas', 'Avatar', '3'), '2011-01-15')\n",
      "(('Elizabeth Thomas', 'Snow White', '5'), '2011-01-19')\n",
      "(('James Cameron', 'Avatar', '5'), '2011-01-20')\n",
      "(('Mike Anderson', 'Gone with the Wind', '3'), '2011-01-09')\n",
      "(('Sarah Martinez', 'Gone with the Wind', '2'), '2011-01-22')\n",
      "(('Sarah Martinez', 'Gone with the Wind', '4'), '2011-01-27')\n",
      "+----------------+--------------------+-----+----------+\n",
      "|            name|               title|stars|ratingDate|\n",
      "+----------------+--------------------+-----+----------+\n",
      "|    Ashley White|                E.T.|    3|2011-01-02|\n",
      "| Brittany Harris|Raiders of the Lo...|    2|2011-01-30|\n",
      "| Brittany Harris|Raiders of the Lo...|    4|2011-01-12|\n",
      "| Brittany Harris|  The Sound of Music|    2|2011-01-20|\n",
      "|   Chris Jackson|                E.T.|    2|2011-01-22|\n",
      "|   Chris Jackson|Raiders of the Lo...|    4|      null|\n",
      "|   Chris Jackson|  The Sound of Music|    3|2011-01-27|\n",
      "|    Daniel Lewis|          Snow White|    4|      null|\n",
      "|Elizabeth Thomas|              Avatar|    3|2011-01-15|\n",
      "|Elizabeth Thomas|          Snow White|    5|2011-01-19|\n",
      "|   James Cameron|              Avatar|    5|2011-01-20|\n",
      "|   Mike Anderson|  Gone with the Wind|    3|2011-01-09|\n",
      "|  Sarah Martinez|  Gone with the Wind|    2|2011-01-22|\n",
      "|  Sarah Martinez|  Gone with the Wind|    4|2011-01-27|\n",
      "+----------------+--------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 05 - Write a query to return the ratings data in a more readable format: \n",
    "# reviewer name, movie title, stars, and ratingDate. Also, sort the data, \n",
    "# first by reviewer name, then by movie title, and lastly by number of stars. \n",
    "\n",
    "movieTrim  = movieRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[1]))\n",
    "reviewerTrim = reviewerRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0], rec[1]))\n",
    "ratingTrim = ratingRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: (rec[0],(rec[1], rec[2], rec[3])))\n",
    "ratingJoinReviewer = reviewerTrim.join(ratingTrim).map(lambda rec: (rec[1][1][0], (rec[1][0], rec[1][1][1], rec[1][1][2])))\n",
    "movieJoinrating = movieTrim.join(ratingJoinReviewer).map(lambda rec: ((rec[1][1][0], rec[1][0], rec[1][1][1]), rec[1][1][2]))\n",
    "\n",
    "Query05RDD = movieJoinrating.sortBy(lambda rec: rec[0], ascending = True).\\\n",
    "sortByKey(ascending = True)\n",
    "for i in Query05RDD.collect(): print(i)\n",
    "\n",
    "    \n",
    "sqlContext.sql(\"select e.name, m.title, r.stars, r.ratingDate from movie m, reviewer e, rating r where \\\n",
    "e.rID = r.rID and r.mID = m.mID order by e.name, m.title, r.stars asc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's the schema: \n",
    "\n",
    "# Movie ( mID, title, year, director ) \n",
    "# English: There is a movie with ID number mID, a title, a release year, and a director. \n",
    "\n",
    "# Reviewer ( rID, name ) \n",
    "# English: The reviewer with ID number rID has a certain name. \n",
    "\n",
    "# Rating ( rID, mID, stars, ratingDate ) \n",
    "# English: The reviewer rID gave the movie mID a number of stars rating (1-5) on a certain ratingDate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'101': 'Gone with the Wind'}\n",
      "{'102': 'Star Wars'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Exception' object has no attribute 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, value, f)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         raise Exception(\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;34m\"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;34m\"action or transformation. RDD transformations and actions can only be invoked by the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-014b197bc10a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmovieBC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#movieBC.value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mbroadcast\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0msent\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0monly\u001b[0m \u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \"\"\"\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickled_broadcast_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sc, value, pickle_registry, path)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jbroadcast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadBroadcastFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickle_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, value, f)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize broadcast: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\": \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Exception' object has no attribute 'message'"
     ]
    }
   ],
   "source": [
    "# Query 06 - For all cases where the same reviewer rated the same movie twice and gave it a higher \n",
    "# rating the second time, return the reviewer's name and the title of the movie.\n",
    "\n",
    "# Broadcasting movie data\n",
    "movieTrim = movieRDD.map(lambda rec: rec.split(\",\")).map(lambda rec: {rec[0] : rec[1]})\n",
    "\n",
    "\n",
    "\n",
    "sqlContext.sql(\"select e.name, m.title from rating r, reviewer e, movie m where \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query 07 - For each movie that has at least one rating, find the highest number of stars that movie received. \n",
    "# Return the movie title and number of stars. Sort by movie title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query 08 - For each movie, return the title and the 'rating spread', that is, \n",
    "# the difference between highest and lowest ratings given to that movie. \n",
    "# Sort by rating spread from highest to lowest, then by movie title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query 09 - Find the difference between the average rating of movies released before 1980 \n",
    "# and the average rating of movies released after 1980. (Make sure to calculate the average rating \n",
    "# for each movie, then the average of those averages for movies before 1980 and movies after. \n",
    "# Don't just calculate the overall average rating before and after 1980.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query 10 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query 11 - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

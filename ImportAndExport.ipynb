{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLACK|SPADE|2\n",
      "BLACK|SPADE|3\n",
      "BLACK|SPADE|4\n",
      "BLACK|SPADE|5\n",
      "BLACK|SPADE|6\n",
      "BLACK|SPADE|7\n",
      "BLACK|SPADE|8\n",
      "BLACK|SPADE|9\n",
      "BLACK|SPADE|10\n",
      "BLACK|SPADE|J\n",
      "***************************************************************************\n",
      "('BLACK', {'CLUB': 13, 'DIAMOND': 0, 'SPADE': 13, 'HEART': 0})\n",
      "('RED', {'CLUB': 0, 'DIAMOND': 13, 'SPADE': 0, 'HEART': 13})\n",
      "***************************************************************************\n",
      "(('BLACK', 'CLUB'), 13)\n",
      "(('BLACK', 'SPADE'), 13)\n",
      "(('RED', 'HEART'), 13)\n",
      "(('RED', 'DIAMOND'), 13)\n",
      "***************************************************************************\n",
      "('10', 4)\n",
      "('4', 4)\n",
      "('CLUB', 13)\n",
      "('SPADE', 13)\n",
      "('J', 4)\n",
      "('BLACK', 26)\n",
      "('K', 4)\n",
      "('RED', 26)\n",
      "('8', 4)\n",
      "('9', 4)\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Extracting data from the textFile\n",
    "sourcePath = \"/Users/pravinkumar/Documents/Spark/testData/cards/\"\n",
    "textRDD = sc.textFile(sourcePath + \"deckofcards.txt\")\n",
    "\n",
    "for i in textRDD.take(10): print(i)\n",
    "print(\"*\"*75)    \n",
    "\n",
    "def fun(x):\n",
    "    spade = 0\n",
    "    club = 0\n",
    "    diamond = 0\n",
    "    heart = 0\n",
    "    for i in x:\n",
    "        if (i == \"SPADE\"):\n",
    "            spade += 1\n",
    "        elif (i == \"CLUB\"):\n",
    "            club += 1\n",
    "        elif (i == \"DIAMOND\"):\n",
    "            diamond += 1\n",
    "        elif (i == \"HEART\"):\n",
    "            heart += 1\n",
    "    return ({\"SPADE\": spade, \"CLUB\": club, \"DIAMOND\": diamond, \"HEART\": heart})\n",
    "\n",
    "# Using GroupByKey        \n",
    "textRDDGroupBy = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: (rec[0], (rec[1]))).\\\n",
    "groupByKey().mapValues(fun)\n",
    "\n",
    "for i in textRDDGroupBy.take(10): print(i)\n",
    "print(\"*\"*75)    \n",
    "\n",
    "# Using ReduceByKey\n",
    "textRDDReduceBy = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: ((rec[0], rec[1]), 1)).\\\n",
    "reduceByKey(lambda agg,count: agg+count)\n",
    "\n",
    "for i in textRDDReduceBy.take(10): print(i)\n",
    "print(\"*\"*75)    \n",
    "\n",
    "# Word count in the file\n",
    "wordCountRDD = textRDD.flatMap(lambda rec: rec.split(\"|\")).map(lambda rec: (rec, 1)).\\\n",
    "reduceByKey(lambda agg, count: agg + count)\n",
    "\n",
    "for i in wordCountRDD.take(10): print(i)\n",
    "print(\"*\"*75)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 'BLACK|SPADE|2')\n",
      "(None, 'BLACK|SPADE|3')\n",
      "(None, 'BLACK|SPADE|4')\n",
      "(None, 'BLACK|SPADE|5')\n",
      "(None, 'BLACK|SPADE|6')\n",
      "(None, 'BLACK|SPADE|7')\n",
      "(None, 'BLACK|SPADE|8')\n",
      "(None, 'BLACK|SPADE|9')\n",
      "(None, 'BLACK|SPADE|10')\n",
      "(None, 'BLACK|SPADE|J')\n",
      "***************************************************************************\n",
      "(None, 'BLACK|SPADE|2')\n",
      "(None, 'BLACK|SPADE|3')\n",
      "(None, 'BLACK|SPADE|4')\n",
      "(None, 'BLACK|SPADE|5')\n",
      "(None, 'BLACK|SPADE|6')\n",
      "(None, 'BLACK|SPADE|7')\n",
      "(None, 'BLACK|SPADE|8')\n",
      "(None, 'BLACK|SPADE|9')\n",
      "(None, 'BLACK|SPADE|10')\n",
      "(None, 'BLACK|SPADE|J')\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating saveAsSequenceFile and reading SequenceFile\n",
    "\n",
    "# Saving the file in SequenceFile Format\n",
    "Destinationpath = \"/Users/pravinkumar/Documents/Spark/Results/\"\n",
    "textRDD.map(lambda x: (None, x)).saveAsSequenceFile(Destinationpath + \"deckofcardsSeq\")\n",
    "\n",
    "# Reading the file in SequenceFile Format\n",
    "SeqRDD = sc.sequenceFile(Destinationpath + \"deckofcardsSeq\")\n",
    "\n",
    "for i in SeqRDD.take(10): print(i)\n",
    "print(\"*\"*75)   \n",
    "\n",
    "# Demonstrating Saving and Reading SequenceFile Using saveAsNewAPIHadoopFile\n",
    "\n",
    "# Saving the file in SequenceFile Format\n",
    "textRDD.map(lambda x: (None, x)).saveAsNewAPIHadoopFile(Destinationpath + \"deckofcardsSeqWithNewAPI\",\"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\",\\\n",
    "                               keyClass=None,valueClass=\"org.apache.hadoop.io.Text\")\n",
    "\n",
    "# Reading the file in SequenceFile Format\n",
    "largedeckSeqWithNewAPI = sc.sequenceFile(Destinationpath + \"deckofcardsSeqWithNewAPI\")\n",
    "\n",
    "for i in largedeckSeqWithNewAPI.take(10): print(i)\n",
    "print(\"*\"*75) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BLACK', 'SPADE', '2']\n",
      "['BLACK', 'SPADE', '3']\n",
      "['BLACK', 'SPADE', '4']\n",
      "['BLACK', 'SPADE', '5']\n",
      "['BLACK', 'SPADE', '6']\n",
      "['BLACK', 'SPADE', '7']\n",
      "['BLACK', 'SPADE', '8']\n",
      "['BLACK', 'SPADE', '9']\n",
      "['BLACK', 'SPADE', '10']\n",
      "['BLACK', 'SPADE', 'J']\n",
      "***************************************************************************\n",
      "Row(color='BLACK', symbol='SPADE', number='2')\n",
      "Row(color='BLACK', symbol='SPADE', number='3')\n",
      "Row(color='BLACK', symbol='SPADE', number='4')\n",
      "Row(color='BLACK', symbol='SPADE', number='5')\n",
      "Row(color='BLACK', symbol='SPADE', number='6')\n",
      "Row(color='BLACK', symbol='SPADE', number='7')\n",
      "Row(color='BLACK', symbol='SPADE', number='8')\n",
      "Row(color='BLACK', symbol='SPADE', number='9')\n",
      "Row(color='BLACK', symbol='SPADE', number='10')\n",
      "Row(color='BLACK', symbol='SPADE', number='J')\n",
      "***************************************************************************\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-----+------+------+\n",
      "|color|number|symbol|\n",
      "+-----+------+------+\n",
      "|BLACK|     2| SPADE|\n",
      "|BLACK|     3| SPADE|\n",
      "|BLACK|     4| SPADE|\n",
      "|BLACK|     5| SPADE|\n",
      "|BLACK|     6| SPADE|\n",
      "|BLACK|     7| SPADE|\n",
      "|BLACK|     8| SPADE|\n",
      "|BLACK|     9| SPADE|\n",
      "|BLACK|    10| SPADE|\n",
      "|BLACK|     J| SPADE|\n",
      "|BLACK|     Q| SPADE|\n",
      "|BLACK|     K| SPADE|\n",
      "|BLACK|     A| SPADE|\n",
      "|BLACK|     2|  CLUB|\n",
      "|BLACK|     3|  CLUB|\n",
      "|BLACK|     4|  CLUB|\n",
      "|BLACK|     5|  CLUB|\n",
      "|BLACK|     6|  CLUB|\n",
      "|BLACK|     7|  CLUB|\n",
      "|BLACK|     8|  CLUB|\n",
      "+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Converting RDD to DF  \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import types\n",
    "#from pyspark.sql.types import StructField\n",
    "\n",
    "# Method 1\n",
    "schema = StructType([\n",
    "    StructField(\"color\", types.StringType(), True),\n",
    "    StructField(\"symbol\", types.StringType(), True),\n",
    "    StructField(\"number\", types.StringType(), True)\n",
    "])\n",
    "textRDDToDF01 = textRDD.map(lambda rec: rec.split(\"|\"))\n",
    "for i in textRDDToDF01.take(10): print(i)\n",
    "print(\"*\"*75)    \n",
    "textDF = sqlContext.createDataFrame(textRDDToDF01, schema)\n",
    "for i in textDF.take(10): print(i)\n",
    "print(\"*\"*75)        \n",
    "\n",
    "# Method 2\n",
    "textRDDToDF02 = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: Row(color= rec[0], symbol= rec[1], \\\n",
    "                                                                            number= rec[2])).toDF()\n",
    "#for i in textDF.take(10): print(i)\n",
    "print(textRDDToDF02.printSchema())\n",
    "textRDDToDF02.show()\n",
    "print(\"*\"*75)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing the file to CSV, JSON, ORC, Parquet from DataFrame\n",
    "\n",
    "textDF.write.csv(Destinationpath + \"deckofcardsCSV\")\n",
    "textDF.write.json(Destinationpath + \"deckofcardsJSON\")\n",
    "textDF.write.orc(Destinationpath + \"deckofcardsORC\")\n",
    "textDF.write.parquet(Destinationpath + \"deckofcardsParquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n",
      "+-----+-----+---+\n",
      "|  _c0|  _c1|_c2|\n",
      "+-----+-----+---+\n",
      "|BLACK|SPADE|  2|\n",
      "|BLACK|SPADE|  3|\n",
      "|BLACK|SPADE|  4|\n",
      "|BLACK|SPADE|  5|\n",
      "|BLACK|SPADE|  6|\n",
      "|BLACK|SPADE|  7|\n",
      "|BLACK|SPADE|  8|\n",
      "|BLACK|SPADE|  9|\n",
      "|BLACK|SPADE| 10|\n",
      "|BLACK|SPADE|  J|\n",
      "|BLACK|SPADE|  Q|\n",
      "|BLACK|SPADE|  K|\n",
      "|BLACK|SPADE|  A|\n",
      "|BLACK| CLUB|  2|\n",
      "|BLACK| CLUB|  3|\n",
      "|BLACK| CLUB|  4|\n",
      "|BLACK| CLUB|  5|\n",
      "|BLACK| CLUB|  6|\n",
      "|BLACK| CLUB|  7|\n",
      "|BLACK| CLUB|  8|\n",
      "+-----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "***************************************************************************\n",
      "['BLACK', '2', 'SPADE']\n",
      "['BLACK', '3', 'SPADE']\n",
      "['BLACK', '4', 'SPADE']\n",
      "['BLACK', '5', 'SPADE']\n",
      "['BLACK', '6', 'SPADE']\n",
      "['BLACK', '7', 'SPADE']\n",
      "['BLACK', '8', 'SPADE']\n",
      "['BLACK', '9', 'SPADE']\n",
      "['BLACK', '10', 'SPADE']\n",
      "['BLACK', 'J', 'SPADE']\n",
      "***************************************************************************\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "Destinationpath = \"/Users/pravinkumar/Documents/Spark/Results/\"\n",
    "\n",
    "largedeckCSVDF = sqlContext.read.csv(Destinationpath + \"deckofcardsCSV\")\n",
    "largedeckCSVDF.printSchema()\n",
    "largedeckCSVDF.show()\n",
    "print(\"*\"*75)\n",
    "\n",
    "#largedeckJSONDF = sqlContext.read.json(Destinationpath + \"deckofcardsJSON\")\n",
    "#largedeckJSONDF.printSchema()\n",
    "#largedeckJSONDF.show()\n",
    "#print(\"*\"*75)\n",
    "\n",
    "# Converting DF to RDD -> list(data)\n",
    "largedeckJSONDF = sqlContext.read.json(Destinationpath + \"deckofcardsJSON\").rdd.map(list)\n",
    "for i in largedeckJSONDF.take(10): print(i)\n",
    "print(\"*\"*75)\n",
    "\n",
    "#largedeckORCDF = sqlContext.read.orc(Destinationpath + \"deckofcardsORC\")\n",
    "#largedeckORCDF.printSchema()\n",
    "#largedeckORCDF.show()\n",
    "#print(\"*\"*75)\n",
    "\n",
    "#largedeckParquetDF = sqlContext.read.parquet(Destinationpath + \"deckofcardsParquet\")\n",
    "#largedeckParquetDF.printSchema()\n",
    "#largedeckParquetDF.show()\n",
    "print(\"*\"*75)\n",
    "\n",
    "#sqlContext.read.format(Destinationpath +)\n",
    "#sqlContext.read.load(Destinationpath +)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

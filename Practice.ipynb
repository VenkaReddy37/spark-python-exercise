{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc.stop()\n",
    "conf = SparkConf().setMaster(\"local[2]\").setAppName(\"Practice\")\n",
    "conf.set(\"spark.driver.cores\", \"1\")\n",
    "conf.set(\"spark.driver.memory\", \"1g\")\n",
    "conf.set(\"spark.executor.memory\",\"1g\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 'BLACK|SPADE|2')\n",
      "(None, 'BLACK|SPADE|3')\n",
      "(None, 'BLACK|SPADE|4')\n",
      "(None, 'BLACK|SPADE|5')\n",
      "(None, 'BLACK|SPADE|6')\n",
      "(None, 'BLACK|SPADE|7')\n",
      "(None, 'BLACK|SPADE|8')\n",
      "(None, 'BLACK|SPADE|9')\n",
      "(None, 'BLACK|SPADE|10')\n",
      "(None, 'BLACK|SPADE|J')\n"
     ]
    }
   ],
   "source": [
    "# Extracting data from the textFile\n",
    "sourcePath = \"/Users/pravinkumar/Documents/Spark/testData/cards/\"\n",
    "textRDD = sc.textFile(sourcePath + \"deckofcards.txt\").map(lambda rec: (None, rec))\n",
    "for i in textRDD.take(10): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Destinationpath = \"/Users/pravinkumar/Documents/Spark/Results/Practice/\"\n",
    "\n",
    "textRDD.repartition(1).saveAsTextFile(Destinationpath + \"DeckTextComp\", \"org.apache.hadoop.io.compress.DefaultCodec\")\n",
    "textRDD.repartition(1).saveAsSequenceFile(Destinationpath + \"DeckSeqComp\", \"org.apache.hadoop.io.compress.DefaultCodec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 'BLACK|SPADE|2')\n",
      "(None, 'BLACK|SPADE|3')\n",
      "(None, 'BLACK|SPADE|4')\n",
      "(None, 'BLACK|SPADE|5')\n",
      "(None, 'BLACK|SPADE|6')\n",
      "(None, 'BLACK|SPADE|7')\n",
      "(None, 'BLACK|SPADE|8')\n",
      "(None, 'BLACK|SPADE|9')\n",
      "(None, 'BLACK|SPADE|10')\n",
      "(None, 'BLACK|SPADE|J')\n",
      "***************************************************************************\n",
      "(None, 'BLACK|SPADE|2')\n",
      "(None, 'BLACK|SPADE|3')\n",
      "(None, 'BLACK|SPADE|4')\n",
      "(None, 'BLACK|SPADE|5')\n",
      "(None, 'BLACK|SPADE|6')\n",
      "(None, 'BLACK|SPADE|7')\n",
      "(None, 'BLACK|SPADE|8')\n",
      "(None, 'BLACK|SPADE|9')\n",
      "(None, 'BLACK|SPADE|10')\n",
      "(None, 'BLACK|SPADE|J')\n"
     ]
    }
   ],
   "source": [
    "seqDeckRDD = sc.sequenceFile(\"/Users/pravinkumar/Documents/Spark/Results/Practice/DeckSeqComp\", \"org.apache.hadoop.io.Text\", \"org.apache.hadoop.io.Text\")\n",
    "for i in seqDeckRDD.take(10): print(i)\n",
    "print(\"*\"*75)\n",
    "textDeckRDD = sc.textFile(\"/Users/pravinkumar/Documents/Spark/Results/Practice/DeckTextComp\")\n",
    "for i in textDeckRDD.take(10): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|color|shade|symbol|\n",
      "+-----+-----+------+\n",
      "|BLACK|SPADE|     2|\n",
      "|BLACK|SPADE|     3|\n",
      "|BLACK|SPADE|     4|\n",
      "|BLACK|SPADE|     5|\n",
      "|BLACK|SPADE|     6|\n",
      "|BLACK|SPADE|     7|\n",
      "|BLACK|SPADE|     8|\n",
      "|BLACK|SPADE|     9|\n",
      "|BLACK|SPADE|    10|\n",
      "|BLACK|SPADE|     J|\n",
      "|BLACK|SPADE|     Q|\n",
      "|BLACK|SPADE|     K|\n",
      "|BLACK|SPADE|     A|\n",
      "|BLACK| CLUB|     2|\n",
      "|BLACK| CLUB|     3|\n",
      "|BLACK| CLUB|     4|\n",
      "|BLACK| CLUB|     5|\n",
      "|BLACK| CLUB|     6|\n",
      "|BLACK| CLUB|     7|\n",
      "|BLACK| CLUB|     8|\n",
      "+-----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD to DF\n",
    "from pyspark.sql import Row\n",
    "textDF = sc.textFile(sourcePath + \"deckofcards.txt\").map(lambda rec: rec.split(\"|\")).\\\n",
    "map(lambda rec: Row(color = rec[0], shade = rec[1], symbol = rec[2])).toDF()\n",
    "\n",
    "textDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textDF.repartition(1).write.csv(Destinationpath + \"DeckCSVComp\", compression = \"org.apache.hadoop.io.compress.DefaultCodec\")\n",
    "\n",
    "sqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "textDF.repartition(1).write.parquet(Destinationpath + \"DeckParqComp\")\n",
    "\n",
    "sqlContext.setConf(\"spark.sql.orc.compression.codec\", \"snappy\")\n",
    "textDF.repartition(1).write.orc(Destinationpath + \"DeckORCComp\")\n",
    "\n",
    "textDF.repartition(1).write.json(Destinationpath + \"DeckJSONComp\", compression = \"org.apache.hadoop.io.compress.DefaultCodec\")\n",
    "textDF.repartition(1).toJSON().saveAsTextFile(Destinationpath + \"DeckTOJSONComp\", \"org.apache.hadoop.io.compress.DefaultCodec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "|  _c0|  _c1|_c2|\n",
      "+-----+-----+---+\n",
      "|BLACK|SPADE|  2|\n",
      "|BLACK|SPADE|  3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+------+\n",
      "|color|shade|symbol|\n",
      "+-----+-----+------+\n",
      "|BLACK|SPADE|     2|\n",
      "|BLACK|SPADE|     3|\n",
      "+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "|color|shade|symbol|\n",
      "+-----+-----+------+\n",
      "|BLACK|SPADE|     2|\n",
      "|BLACK|SPADE|     3|\n",
      "+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "|color|shade|symbol|\n",
      "+-----+-----+------+\n",
      "|BLACK|SPADE|     2|\n",
      "|BLACK|SPADE|     3|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DFFromCSV = sqlContext.read.csv(Destinationpath + \"DeckCSVComp\")\n",
    "DFFromCSV.limit(2).show()\n",
    "DFFromParq = sqlContext.read.parquet(Destinationpath + \"DeckParqComp\")\n",
    "DFFromParq.limit(2).show()\n",
    "DFFromORC = sqlContext.read.orc(Destinationpath + \"DeckORCComp\")\n",
    "DFFromORC.limit(2).show()\n",
    "DFFromJson = sqlContext.read.json(Destinationpath + \"DeckJSONComp\")\n",
    "DFFromJson.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 : 4\n",
      "CLUB : 13\n",
      "5 : 4\n",
      "SPADE : 13\n",
      "3 : 4\n",
      "2 : 4\n",
      "BLACK : 26\n",
      "7 : 4\n",
      "8 : 4\n",
      "6 : 4\n",
      "4 : 4\n",
      "9 : 4\n",
      "DIAMOND : 13\n",
      "K : 4\n",
      "Q : 4\n",
      "J : 4\n",
      "RED : 26\n",
      "HEART : 13\n",
      "A : 4\n",
      "('10', 4)\n",
      "('4', 4)\n",
      "('CLUB', 13)\n",
      "('SPADE', 13)\n",
      "('J', 4)\n",
      "('BLACK', 26)\n",
      "('K', 4)\n",
      "('RED', 26)\n",
      "('8', 4)\n",
      "('9', 4)\n"
     ]
    }
   ],
   "source": [
    "# WOrdcount\n",
    "wordCountReduceByRDD = sc.textFile(sourcePath + \"deckofcards.txt\").map(lambda rec: rec.split(\"|\")).flatMap(lambda rec: rec).\\\n",
    "map(lambda rec: (rec, 1)).reduceByKey(lambda Acc, Value: Acc + Value)\n",
    "wordCountRDD = sc.textFile(sourcePath + \"deckofcards.txt\").map(lambda rec: rec.split(\"|\")).flatMap(lambda rec: rec).\\\n",
    "map(lambda rec: (rec, 1)).countByKey()\n",
    "for k, v in wordCountRDD.items(): print(\"{} : {}\".format(k, v))\n",
    "for i in wordCountReduceByRDD.take(10): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

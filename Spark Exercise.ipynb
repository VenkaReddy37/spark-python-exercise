{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing required dependencies\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10d94aba8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting data from the textFile\n",
    "sourcePath = \"/Users/pravinkumar/Documents/Spark/testData/weblogs\"\n",
    "wholeTextRDD = sc.wholeTextFiles(sourcePath)\n",
    "#for i in wholeTextRDD.take(10): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLACK|SPADE|2\n",
      "BLACK|SPADE|3\n",
      "BLACK|SPADE|4\n",
      "BLACK|SPADE|5\n",
      "BLACK|SPADE|6\n",
      "BLACK|SPADE|7\n",
      "BLACK|SPADE|8\n",
      "BLACK|SPADE|9\n",
      "BLACK|SPADE|10\n",
      "BLACK|SPADE|J\n",
      "***************************************************************************\n",
      "('BLACK', ('CLUB', 13))\n",
      "('BLACK', ('DIAMOND', 0))\n",
      "('BLACK', ('SPADE', 13))\n",
      "('BLACK', ('HEART', 0))\n",
      "('RED', ('CLUB', 0))\n",
      "('RED', ('DIAMOND', 13))\n",
      "('RED', ('SPADE', 0))\n",
      "('RED', ('HEART', 13))\n",
      "***************************************************************************\n",
      "(('BLACK', 'CLUB'), 13)\n",
      "(('BLACK', 'SPADE'), 13)\n",
      "(('RED', 'HEART'), 13)\n",
      "(('RED', 'DIAMOND'), 13)\n",
      "***************************************************************************\n",
      "(('BLACK', 'CLUB'), 13)\n",
      "(('BLACK', 'SPADE'), 13)\n",
      "(('RED', 'HEART'), 13)\n",
      "(('RED', 'DIAMOND'), 13)\n",
      "(('BLACK', 'CLUB'), (13, '1111111111111', 13.0))\n",
      "(('BLACK', 'SPADE'), (13, '1111111111111', 13.0))\n",
      "(('RED', 'HEART'), (13, '1111111111111', 13.0))\n",
      "(('RED', 'DIAMOND'), (13, '1111111111111', 13.0))\n"
     ]
    }
   ],
   "source": [
    "# Extracting data from the textFile\n",
    "sourcePath = \"/Users/pravinkumar/Documents/Spark/testData/cards/\"\n",
    "textRDD = sc.textFile(sourcePath + \"deckofcards.txt\")\n",
    "\n",
    "for i in textRDD.take(10): print(i)\n",
    "print(\"*\"*75)    \n",
    "\n",
    "def fun(x):\n",
    "    spade = 0\n",
    "    club = 0\n",
    "    diamond = 0\n",
    "    heart = 0\n",
    "    for i in x:\n",
    "        if (i == \"SPADE\"):\n",
    "            spade += 1\n",
    "        elif (i == \"CLUB\"):\n",
    "            club += 1\n",
    "        elif (i == \"DIAMOND\"):\n",
    "            diamond += 1\n",
    "        elif (i == \"HEART\"):\n",
    "            heart += 1\n",
    "    return ({\"SPADE\": spade, \"CLUB\": club, \"DIAMOND\": diamond, \"HEART\": heart})\n",
    "\n",
    "# Using GroupByKey        \n",
    "textRDDGroupBy = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: (rec[0], (rec[1]))).\\\n",
    "groupByKey().mapValues(fun).flatMapValues(lambda rec: [(i, rec[i]) for i in rec.keys()])\n",
    "\n",
    "for i in textRDDGroupBy.take(10): print(i)\n",
    "print(\"*\"*75)    \n",
    "\n",
    "# Using ReduceByKey\n",
    "textRDDReduceBy = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: ((rec[0], rec[1]), 1)).\\\n",
    "reduceByKey(lambda agg,value: agg + value)\n",
    "for i in textRDDReduceBy.take(10): print(i)\n",
    "print(\"*\"*75)\n",
    "\n",
    "# Using AggregateByKey\n",
    "seqOp = (lambda agg, val: agg + val)\n",
    "combOp = (lambda agg, val: agg + val)\n",
    "textRDDAggBy = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: ((rec[0], rec[1]), 1)).\\\n",
    "aggregateByKey(0, seqOp, combOp)                                                           \n",
    "for i in textRDDAggBy.take(10): print(i)\n",
    "                                                           \n",
    "# Using ReduceByKey with two values\n",
    "reduceByForFun = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: ((rec[0], rec[1]), (\"1\",1.0))).\\\n",
    "reduceByKey(lambda agg,value: (agg[0]+\"(0_0)\"+value[0], agg[1]+(value[1]*2.2)))\n",
    "\n",
    "# Using AggregateByKey with two values\n",
    "seqOp = (lambda agg, val: (agg[0] + 1, agg[1] + val[0], agg[2] + val[1]))\n",
    "combOp = (lambda agg, val: (agg[0] + val[0], agg[1] + \"(0_0)\"+ val[1], agg[2] / val[2]))\n",
    "aggregateByForFun = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: ((rec[0], rec[1]), (\"1\",1.0))).\\\n",
    "aggregateByKey((0, \"\", 0.0), seqOp, combOp)\n",
    "\n",
    "for i in aggregateByForFun.take(10): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLACK|SPADE|2\n",
      "BLACK|SPADE|3\n",
      "BLACK|SPADE|4\n",
      "BLACK|SPADE|5\n",
      "BLACK|SPADE|6\n",
      "BLACK|SPADE|7\n",
      "BLACK|SPADE|8\n",
      "BLACK|SPADE|9\n",
      "BLACK|SPADE|10\n",
      "BLACK|SPADE|J\n"
     ]
    }
   ],
   "source": [
    "head = textRDD.take(10)\n",
    "for i in head:print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10', 4)\n",
      "('4', 4)\n",
      "('CLUB', 13)\n",
      "('SPADE', 13)\n",
      "('J', 4)\n",
      "('BLACK', 26)\n",
      "('K', 4)\n",
      "('RED', 26)\n",
      "('8', 4)\n",
      "('9', 4)\n",
      "defaultdict(<class 'int'>, {'10': 4, 'CLUB': 13, '5': 4, 'SPADE': 13, '3': 4, '2': 4, 'BLACK': 26, '7': 4, '8': 4, '6': 4, '4': 4, '9': 4, 'DIAMOND': 13, 'K': 4, 'Q': 4, 'J': 4, 'RED': 26, 'HEART': 13, 'A': 4})\n",
      "'10' : 4\n",
      "'CLUB' : 13\n",
      "'5' : 4\n",
      "'SPADE' : 13\n",
      "'3' : 4\n",
      "'2' : 4\n",
      "'BLACK' : 26\n",
      "'7' : 4\n",
      "'8' : 4\n",
      "'6' : 4\n",
      "'4' : 4\n",
      "'9' : 4\n",
      "'DIAMOND' : 13\n",
      "'K' : 4\n",
      "'Q' : 4\n",
      "'J' : 4\n",
      "'RED' : 26\n",
      "'HEART' : 13\n",
      "'A' : 4\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Word count in the file\n",
    "wordCountRDD = textRDD.flatMap(lambda rec: rec.split(\"|\")).map(lambda rec: (rec, 1)).\\\n",
    "reduceByKey(lambda agg, value: agg + value)\n",
    "for i in wordCountRDD.take(10): print(i)\n",
    "    \n",
    "wordCountRDD = textRDD.flatMap(lambda rec: rec.split(\"|\")).map(lambda rec: (rec, 1)).\\\n",
    "countByKey()\n",
    "\n",
    "print(wordCountRDD)\n",
    "for k, v in wordCountRDD.items():\n",
    "    print(\"'{}' : {}\".format(k, v))\n",
    "print(\"*\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLACK|SPADE|2\n",
      "BLACK|SPADE|3\n",
      "BLACK|SPADE|4\n",
      "BLACK|SPADE|5\n",
      "BLACK|SPADE|6\n",
      "BLACK|SPADE|7\n",
      "BLACK|SPADE|8\n",
      "BLACK|SPADE|9\n",
      "BLACK|SPADE|10\n",
      "BLACK|SPADE|J\n",
      "676\n"
     ]
    }
   ],
   "source": [
    "# Performing Join operation\n",
    "for i in textRDD.take(10): print(i)\n",
    "leftRDD = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: (rec[1], rec[0]))\n",
    "rightRDD = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: (rec[1], rec[2]))\n",
    "\n",
    "count = sc.accumulator(0)\n",
    "\n",
    "resultJoinRDD = leftRDD.join(rightRDD)\n",
    "\n",
    "def f(x):\n",
    "    global count\n",
    "    count += 1\n",
    "\n",
    "resultJoinRDD.foreach(f)\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD to DF conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|color|shape|number|\n",
      "+-----+-----+------+\n",
      "|BLACK|SPADE|     2|\n",
      "|BLACK|SPADE|     3|\n",
      "|BLACK|SPADE|     4|\n",
      "|BLACK|SPADE|     5|\n",
      "|BLACK|SPADE|     6|\n",
      "|BLACK|SPADE|     7|\n",
      "|BLACK|SPADE|     8|\n",
      "|BLACK|SPADE|     9|\n",
      "|BLACK|SPADE|    10|\n",
      "|BLACK|SPADE|     J|\n",
      "|BLACK|SPADE|     Q|\n",
      "|BLACK|SPADE|     K|\n",
      "|BLACK|SPADE|     A|\n",
      "|BLACK| CLUB|     2|\n",
      "|BLACK| CLUB|     3|\n",
      "|BLACK| CLUB|     4|\n",
      "|BLACK| CLUB|     5|\n",
      "|BLACK| CLUB|     6|\n",
      "|BLACK| CLUB|     7|\n",
      "|BLACK| CLUB|     8|\n",
      "+-----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "***************************************************************************\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      " |-- shape: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-----+------+-----+\n",
      "|color|number|shape|\n",
      "+-----+------+-----+\n",
      "|BLACK|     2|SPADE|\n",
      "|BLACK|     3|SPADE|\n",
      "|BLACK|     4|SPADE|\n",
      "|BLACK|     5|SPADE|\n",
      "|BLACK|     6|SPADE|\n",
      "|BLACK|     7|SPADE|\n",
      "|BLACK|     8|SPADE|\n",
      "|BLACK|     9|SPADE|\n",
      "|BLACK|    10|SPADE|\n",
      "|BLACK|     J|SPADE|\n",
      "|BLACK|     Q|SPADE|\n",
      "|BLACK|     K|SPADE|\n",
      "|BLACK|     A|SPADE|\n",
      "|BLACK|     2| CLUB|\n",
      "|BLACK|     3| CLUB|\n",
      "|BLACK|     4| CLUB|\n",
      "|BLACK|     5| CLUB|\n",
      "|BLACK|     6| CLUB|\n",
      "|BLACK|     7| CLUB|\n",
      "|BLACK|     8| CLUB|\n",
      "+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "***************************************************************************\n",
      "+-----+------+\n",
      "|color|number|\n",
      "+-----+------+\n",
      "|BLACK|     2|\n",
      "|BLACK|     3|\n",
      "|BLACK|     4|\n",
      "|BLACK|     5|\n",
      "|BLACK|     6|\n",
      "|BLACK|     7|\n",
      "|BLACK|     8|\n",
      "|BLACK|     9|\n",
      "|BLACK|    10|\n",
      "|BLACK|     J|\n",
      "|BLACK|     Q|\n",
      "|BLACK|     K|\n",
      "|BLACK|     A|\n",
      "|BLACK|     2|\n",
      "|BLACK|     3|\n",
      "|BLACK|     4|\n",
      "|BLACK|     5|\n",
      "|BLACK|     6|\n",
      "|BLACK|     7|\n",
      "|BLACK|     8|\n",
      "+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting RDD to DF  \n",
    "\n",
    "# Method 1\n",
    "schema = StructType([\n",
    "    StructField(\"color\", types.StringType(), True),\n",
    "    StructField(\"shape\", types.StringType(), True),\n",
    "    StructField(\"number\", types.StringType(), True)\n",
    "])\n",
    "textRDDToDF01 = textRDD.map(lambda rec: rec.split(\"|\"))   \n",
    "textDF = sqlContext.createDataFrame(textRDDToDF01, schema)\n",
    "textDF.show()\n",
    "print(\"*\"*75)        \n",
    "\n",
    "# Method 2\n",
    "textRDDToDF02 = textRDD.map(lambda rec: rec.split(\"|\")).map(lambda rec: Row(color= rec[0], shape= rec[1], \\\n",
    "                                                                            number= rec[2])).toDF()\n",
    "#for i in textDF.take(10): print(i)\n",
    "print(textRDDToDF02.printSchema())\n",
    "textRDDToDF02.show()\n",
    "print(\"*\"*75)\n",
    "\n",
    "# Getting only the required columns\n",
    "textRDDToDF02.select(\"color\", \"number\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', 'shape', 'number']\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "# Getting the column details\n",
    "print(textDF.columns)\n",
    "# getting count of records\n",
    "print(textDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|color|count(1)|\n",
      "+-----+--------+\n",
      "|  RED|      26|\n",
      "|BLACK|      26|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping operation on DF\n",
    "textDF.groupBy(textDF.color).agg({\"*\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+\n",
      "|color|  shape|number|\n",
      "+-----+-------+------+\n",
      "|BLACK|  SPADE|     2|\n",
      "|BLACK|   CLUB|     2|\n",
      "|  RED|DIAMOND|     2|\n",
      "|  RED|  HEART|     2|\n",
      "+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering operation on DF\n",
    "textDF.filter(textDF.number == \"2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "+-----+-------+------+\n",
      "|color|  shape|number|\n",
      "+-----+-------+------+\n",
      "|  RED|DIAMOND|     2|\n",
      "|  RED|DIAMOND|     3|\n",
      "|  RED|DIAMOND|     4|\n",
      "|  RED|DIAMOND|     5|\n",
      "|  RED|DIAMOND|     6|\n",
      "|  RED|DIAMOND|     7|\n",
      "|  RED|DIAMOND|     8|\n",
      "|  RED|DIAMOND|     9|\n",
      "|  RED|DIAMOND|    10|\n",
      "|  RED|DIAMOND|     J|\n",
      "|  RED|DIAMOND|     Q|\n",
      "|  RED|DIAMOND|     K|\n",
      "|  RED|DIAMOND|     A|\n",
      "|  RED|DIAMOND|     2|\n",
      "|  RED|DIAMOND|     3|\n",
      "|  RED|DIAMOND|     4|\n",
      "|  RED|DIAMOND|     5|\n",
      "|  RED|DIAMOND|     6|\n",
      "|  RED|DIAMOND|     7|\n",
      "|  RED|DIAMOND|     8|\n",
      "+-----+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining two tables on a particular column\n",
    "textLeft = textDF.select(\"color\", \"shape\")\n",
    "textRight = textDF.select(\"shape\", \"number\")\n",
    "\n",
    "resultJoin = textLeft.join(textRight, textLeft.shape == textRight.shape).drop(textRight.shape)\n",
    "# getting count of records after joining\n",
    "# its maching each shape of textLeft to each shape of textRight which results in 52*13, so be carfull with this operation\n",
    "print(resultJoin.count())\n",
    "resultJoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|color|shape|number|\n",
      "+-----+-----+------+\n",
      "|BLACK|SPADE|     2|\n",
      "|BLACK|SPADE|     3|\n",
      "|BLACK|SPADE|     4|\n",
      "|BLACK|SPADE|     5|\n",
      "|BLACK|SPADE|     6|\n",
      "|BLACK|SPADE|     7|\n",
      "|BLACK|SPADE|     8|\n",
      "|BLACK|SPADE|     9|\n",
      "|BLACK|SPADE|    10|\n",
      "|BLACK|SPADE|     J|\n",
      "|BLACK|SPADE|     Q|\n",
      "|BLACK|SPADE|     K|\n",
      "|BLACK|SPADE|     A|\n",
      "|BLACK| CLUB|     2|\n",
      "|BLACK| CLUB|     3|\n",
      "|BLACK| CLUB|     4|\n",
      "|BLACK| CLUB|     5|\n",
      "|BLACK| CLUB|     6|\n",
      "|BLACK| CLUB|     7|\n",
      "|BLACK| CLUB|     8|\n",
      "+-----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "['BLACK', 'SPADE', '2']\n",
      "['BLACK', 'SPADE', '3']\n",
      "['BLACK', 'SPADE', '4']\n",
      "['BLACK', 'SPADE', '5']\n",
      "['BLACK', 'SPADE', '6']\n",
      "['BLACK', 'SPADE', '7']\n",
      "['BLACK', 'SPADE', '8']\n",
      "['BLACK', 'SPADE', '9']\n",
      "['BLACK', 'SPADE', '10']\n",
      "['BLACK', 'SPADE', 'J']\n"
     ]
    }
   ],
   "source": [
    "# Registering DF as a Temp Table and performing queries\n",
    "textDF.registerTempTable(\"data\")\n",
    "sqlContext.getConf(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "\n",
    "query = sqlContext.sql(\"select * from data\")\n",
    "query.show()\n",
    "\n",
    "# Converting DF to RDD and printing the data\n",
    "for i in query.rdd.map(list).take(10): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing & Exporting in Different File Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa'), (3, 'aaa')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x)) \n",
    "rdd.saveAsSequenceFile(\"/Users/pravinkumar/Documents/Spark/Results/path/to/file\")\n",
    "sorted(sc.sequenceFile(\"/Users/pravinkumar/Documents/Spark/Results/path/to/file\", \"org.apache.hadoop.io.IntWritable\", \"org.apache.hadoop.io.Text\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 'BLACK|SPADE|2')\n",
      "(None, 'BLACK|SPADE|3')\n",
      "(None, 'BLACK|SPADE|4')\n",
      "(None, 'BLACK|SPADE|5')\n",
      "(None, 'BLACK|SPADE|6')\n",
      "(None, 'BLACK|SPADE|7')\n",
      "(None, 'BLACK|SPADE|8')\n",
      "(None, 'BLACK|SPADE|9')\n",
      "(None, 'BLACK|SPADE|10')\n",
      "(None, 'BLACK|SPADE|J')\n",
      "***************************************************************************\n",
      "(None, 'BLACK|SPADE|2')\n",
      "(None, 'BLACK|SPADE|3')\n",
      "(None, 'BLACK|SPADE|4')\n",
      "(None, 'BLACK|SPADE|5')\n",
      "(None, 'BLACK|SPADE|6')\n",
      "(None, 'BLACK|SPADE|7')\n",
      "(None, 'BLACK|SPADE|8')\n",
      "(None, 'BLACK|SPADE|9')\n",
      "(None, 'BLACK|SPADE|10')\n",
      "(None, 'BLACK|SPADE|J')\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating saveAsSequenceFile and reading SequenceFile\n",
    "\n",
    "# Saving the file in SequenceFile Format\n",
    "Destinationpath = \"/Users/pravinkumar/Documents/Spark/Results/\"\n",
    "textRDD.map(lambda x: (None, x)).saveAsSequenceFile(Destinationpath + \"deckofcardsSeq\")\n",
    "\n",
    "# Reading the file in SequenceFile Format\n",
    "SeqRDD = sc.sequenceFile(Destinationpath + \"deckofcardsSeq\")\n",
    "\n",
    "for i in SeqRDD.take(10): print(i)\n",
    "print(\"*\"*75)   \n",
    "\n",
    "# Demonstrating Saving and Reading SequenceFile Using saveAsNewAPIHadoopFile\n",
    "\n",
    "# Saving the file in SequenceFile Format\n",
    "textRDD.map(lambda x: (None, x)).saveAsNewAPIHadoopFile(Destinationpath + \"deckofcardsSeqWithNewAPI\",\"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\",\\\n",
    "                               keyClass=None,valueClass=\"org.apache.hadoop.io.Text\")\n",
    "\n",
    "# Reading the file in SequenceFile Format\n",
    "largedeckSeqWithNewAPI = sc.sequenceFile(Destinationpath + \"deckofcardsSeqWithNewAPI\")\n",
    "\n",
    "for i in largedeckSeqWithNewAPI.take(10): print(i)\n",
    "print(\"*\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing the file to CSV, JSON, ORC, Parquet from DataFrame\n",
    "\n",
    "textDF.write.csv(Destinationpath + \"deckofcardsCSV\")\n",
    "textDF.write.json(Destinationpath + \"deckofcardsJSON\")\n",
    "textDF.write.orc(Destinationpath + \"deckofcardsORC\")\n",
    "textDF.write.parquet(Destinationpath + \"deckofcardsParquet\")\n",
    "textDF.write.format(\"json\").save(Destinationpath + \"deckofcardsJSONThroughFormat\")\n",
    "textDF.write.save(Destinationpath + \"deckofcardsParquetThroughFormat\", format = \"parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n",
      "+-----+-----+---+\n",
      "|  _c0|  _c1|_c2|\n",
      "+-----+-----+---+\n",
      "|BLACK|SPADE|  2|\n",
      "|BLACK|SPADE|  3|\n",
      "|BLACK|SPADE|  4|\n",
      "|BLACK|SPADE|  5|\n",
      "|BLACK|SPADE|  6|\n",
      "|BLACK|SPADE|  7|\n",
      "|BLACK|SPADE|  8|\n",
      "|BLACK|SPADE|  9|\n",
      "|BLACK|SPADE| 10|\n",
      "|BLACK|SPADE|  J|\n",
      "|BLACK|SPADE|  Q|\n",
      "|BLACK|SPADE|  K|\n",
      "|BLACK|SPADE|  A|\n",
      "|BLACK| CLUB|  2|\n",
      "|BLACK| CLUB|  3|\n",
      "|BLACK| CLUB|  4|\n",
      "|BLACK| CLUB|  5|\n",
      "|BLACK| CLUB|  6|\n",
      "|BLACK| CLUB|  7|\n",
      "|BLACK| CLUB|  8|\n",
      "+-----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "***************************************************************************\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      " |-- shape: string (nullable = true)\n",
      "\n",
      "+-----+------+-----+\n",
      "|color|number|shape|\n",
      "+-----+------+-----+\n",
      "|BLACK|     2|SPADE|\n",
      "|BLACK|     3|SPADE|\n",
      "|BLACK|     4|SPADE|\n",
      "|BLACK|     5|SPADE|\n",
      "|BLACK|     6|SPADE|\n",
      "|BLACK|     7|SPADE|\n",
      "|BLACK|     8|SPADE|\n",
      "|BLACK|     9|SPADE|\n",
      "|BLACK|    10|SPADE|\n",
      "|BLACK|     J|SPADE|\n",
      "|BLACK|     Q|SPADE|\n",
      "|BLACK|     K|SPADE|\n",
      "|BLACK|     A|SPADE|\n",
      "|BLACK|     2| CLUB|\n",
      "|BLACK|     3| CLUB|\n",
      "|BLACK|     4| CLUB|\n",
      "|BLACK|     5| CLUB|\n",
      "|BLACK|     6| CLUB|\n",
      "|BLACK|     7| CLUB|\n",
      "|BLACK|     8| CLUB|\n",
      "+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "***************************************************************************\n",
      "['BLACK', '2', 'SPADE']\n",
      "['BLACK', '3', 'SPADE']\n",
      "['BLACK', '4', 'SPADE']\n",
      "['BLACK', '5', 'SPADE']\n",
      "['BLACK', '6', 'SPADE']\n",
      "['BLACK', '7', 'SPADE']\n",
      "['BLACK', '8', 'SPADE']\n",
      "['BLACK', '9', 'SPADE']\n",
      "['BLACK', '10', 'SPADE']\n",
      "['BLACK', 'J', 'SPADE']\n",
      "***************************************************************************\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- shape: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      "\n",
      "+-----+-----+------+\n",
      "|color|shape|number|\n",
      "+-----+-----+------+\n",
      "|BLACK|SPADE|     2|\n",
      "|BLACK|SPADE|     3|\n",
      "|BLACK|SPADE|     4|\n",
      "|BLACK|SPADE|     5|\n",
      "|BLACK|SPADE|     6|\n",
      "|BLACK|SPADE|     7|\n",
      "|BLACK|SPADE|     8|\n",
      "|BLACK|SPADE|     9|\n",
      "|BLACK|SPADE|    10|\n",
      "|BLACK|SPADE|     J|\n",
      "|BLACK|SPADE|     Q|\n",
      "|BLACK|SPADE|     K|\n",
      "|BLACK|SPADE|     A|\n",
      "|BLACK| CLUB|     2|\n",
      "|BLACK| CLUB|     3|\n",
      "|BLACK| CLUB|     4|\n",
      "|BLACK| CLUB|     5|\n",
      "|BLACK| CLUB|     6|\n",
      "|BLACK| CLUB|     7|\n",
      "|BLACK| CLUB|     8|\n",
      "+-----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "***************************************************************************\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- shape: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      "\n",
      "+-----+-----+------+\n",
      "|color|shape|number|\n",
      "+-----+-----+------+\n",
      "|BLACK|SPADE|     2|\n",
      "|BLACK|SPADE|     3|\n",
      "|BLACK|SPADE|     4|\n",
      "|BLACK|SPADE|     5|\n",
      "|BLACK|SPADE|     6|\n",
      "|BLACK|SPADE|     7|\n",
      "|BLACK|SPADE|     8|\n",
      "|BLACK|SPADE|     9|\n",
      "|BLACK|SPADE|    10|\n",
      "|BLACK|SPADE|     J|\n",
      "|BLACK|SPADE|     Q|\n",
      "|BLACK|SPADE|     K|\n",
      "|BLACK|SPADE|     A|\n",
      "|BLACK| CLUB|     2|\n",
      "|BLACK| CLUB|     3|\n",
      "|BLACK| CLUB|     4|\n",
      "|BLACK| CLUB|     5|\n",
      "|BLACK| CLUB|     6|\n",
      "|BLACK| CLUB|     7|\n",
      "|BLACK| CLUB|     8|\n",
      "+-----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "Destinationpath = \"/Users/pravinkumar/Documents/Spark/Results/\"\n",
    "\n",
    "# Loading file in CSV format\n",
    "largedeckCSVDF = sqlContext.read.csv(Destinationpath + \"deckofcardsCSV\")\n",
    "largedeckCSVDF = sqlContext.read.load(Destinationpath + \"deckofcardsCSV\", format = \"csv\")\n",
    "largedeckCSVDF.printSchema()\n",
    "largedeckCSVDF.show()\n",
    "print(\"*\"*75)\n",
    "\n",
    "# Loading file in JSON format\n",
    "largedeckJSONDF = sqlContext.read.json(Destinationpath + \"deckofcardsJSON\")\n",
    "largedeckJSONDF.printSchema()\n",
    "largedeckJSONDF.show()\n",
    "print(\"*\"*75)\n",
    "\n",
    "# Converting DF to RDD -> list(data)\n",
    "largedeckJSONDF = sqlContext.read.json(Destinationpath + \"deckofcardsJSON\").rdd.map(list)\n",
    "for i in largedeckJSONDF.take(10): print(i)\n",
    "print(\"*\"*75)\n",
    "\n",
    "# Loading file in ORC format\n",
    "largedeckORCDF = sqlContext.read.orc(Destinationpath + \"deckofcardsORC\")\n",
    "largedeckORCDF.printSchema()\n",
    "largedeckORCDF.show()\n",
    "print(\"*\"*75)\n",
    "\n",
    "# Loading file in Parquet format\n",
    "largedeckParquetDF = sqlContext.read.parquet(Destinationpath + \"deckofcardsParquet\")\n",
    "largedeckParquetDF.printSchema()\n",
    "largedeckParquetDF.show()\n",
    "print(\"*\"*75)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Failed to find data source: com.databricks.spark.avro. Please find an Avro package at http://spark.apache.org/third-party-projects.html;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o502.save.\n: org.apache.spark.sql.AnalysisException: Failed to find data source: com.databricks.spark.avro. Please find an Avro package at http://spark.apache.org/third-party-projects.html;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:590)\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:86)\n\tat org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:86)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:516)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-25d04b415951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The Avro records get converted to Spark types, filtered, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# then written back out as Avro records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtextDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.databricks.spark.avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDestinationpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"deckofcardsAVRO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlargedeckAVRODF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.databricks.spark.avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDestinationpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"deckofcardsAVRO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pravinkumar/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Failed to find data source: com.databricks.spark.avro. Please find an Avro package at http://spark.apache.org/third-party-projects.html;'"
     ]
    }
   ],
   "source": [
    "# import needed for the .avro method to be added\n",
    "\n",
    "\n",
    "# The Avro records get converted to Spark types, filtered, and\n",
    "# then written back out as Avro records\n",
    "textDF.write.format(\"com.databricks.spark.avro\").save(Destinationpath + \"deckofcardsAVRO\")\n",
    "largedeckAVRODF = spark.read.format(\"com.databricks.spark.avro\").load(Destinationpath + \"deckofcardsAVRO\")\n",
    "\n",
    "largedeckAVRODF.printSchema()\n",
    "largedeckAVRODF.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
